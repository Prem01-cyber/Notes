
***
###### `Title :- Crawling`
###### `Created on :- 2023-05-30 - 07:11`
###### `Created by:- Prem J`
***
#### `Brief -->`

- Crawling a website is the systematic or automatic process of exploring a website to **list all of the resources** encountered along the way..
- We use the crawling process to find as many pages and sub directories belonging to a website as possible.

#### `Advantages -->`

- It shows us the structure of the website we are auditing and an overview of the attack surface.

#### `How to -->`

- there are tools that perform crawling for us some of them are ZAP, FFuF, 

### 1. ZAP (Zed Attack Proxy) -->

- [Zed Attack Proxy](https://www.zaproxy.org/) (`ZAP`) is an open-source web proxy that belongs to the [Open Web Application Security Project](https://owasp.org/) (`OWASP`). It allows us to perform **manual and automated** security testing on web applications. Using it as a proxy server will enable us to intercept and manipulate all the traffic that passes through it.
- We can use the **spidering functionality** to perform crawling.

>[!Note]
>One handy feature of ZAP is the built-in Fuzzer and Manual Request Editor. We can send any request to them to alter it manually or fuzz it with a list of payloads by right-clicking on the request and using the menu "Open/Resend with Request Editor..." or the "Fuzz..." submenu under the Attack menu.

- [[burpsuite]] is similar to OWASP ZAP, but it's a paid version of it. 

#### `Disadvantages -->`

1. ZAP spidering modules only enumerates the resources it finds in links and forms, but it can miss important information such as hidden folders or backup files.

#### `Alternatives -->`

1. ffuf can be used to discover hidden files and folders that can't be spotted just by simply browsing the website.

### 2. FFuF -->

- FFuF can be used to perform spidering, by giving a list of folder names and instruct it to look recursively.

>`ffuf -recursion -recursion-depth 1 -u http://192.168.10.10/FUZZ -w wordlist.txt`

![[Screenshot from 2023-05-30 07-42-43.png]]
- We can see in the image how `ffuf` creates **new jobs** for every detected folder. This task can be very **resource-intensive** for the target server. If the website responds slower than usual, we can lower the rate of requests using the `-rate` parameter.

## Sensitive Information Disclosure

- It is typical for the webserver and the web application to handle files it needs to function, it is common to find backup[^1] or unreferenced files that can have some confidential information or credentials.
- There are some lists of common extensions we can find in the raft-[ small | medium | large ]-extensions.txt files from [SecLists](https://github.com/danielmiessler/SecLists/tree/master/Discovery/Web-Content).

#### `How to -->`

- We will combine some of the folders we have found before, a list of common extensions and some words extracted from website using [CeWL](https://github.com/digininja/CeWL) to see if we can find something that should not be there.

![[Screenshot from 2023-05-30 10-19-52.png]]

- The next step will be to combine everything in ffuf to see if we can find some juicy information. For this, we will use the following parameters in `ffuf`:

- `-w`: We separate the word lists by coma and add an alias to them to inject them as fuzzing points later
- `-u`: Our target URL with the fuzzing points.

>`ffuf -w ./folders.txt:FOLDERS,./wordlist.txt:WORDLIST,./extensions.txt:EXTENSIONS -u http://192.168.10.10/FOLDERS/WORDLISTEXTENSIONS`

![[Screenshot from 2023-05-30 10-21-39.png]]

- now we can use curl to request the secret file

>`curl http://192.168.10.10/wp-content/secret~`

[^1]: Backup files are generated by creating snapshots, different versions of a file, or from a text editor without developers knowledge